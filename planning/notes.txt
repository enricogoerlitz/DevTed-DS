PLOT
    -> default plot settings
        - change with dt.plot.set(...) / dt.plot.get_setting() -> alle settings printen [TDD_TEST]

    -> [wichtigsten Params übernehmen und title[CAPSLOC] -> default df_nameCAPSLOC, xlabel, ylabel hinzufügen] [TDD_TEST -> keine Fehler treten auf! bzw. instance checken!]
        - histplot
        - countplot
        - pointplot
        - scatterplot
        - barplot (incl. stacked)
        - lineplot
        - regplot & lmplot in one!
        - kdeplot
    
    MORE FEATURES->
        + sorted -> bool (by values, by categoriesArr ...)
        + figsize + figsize_cat=[small, medium, large, xlarge]
        + rotate x/y
        + order
        + show Data-Table (bool=False!)
    
    -> hinzufügen
        - kde plot with hue (like in Titanic Facitgrid)
        - barhplot (incl. stacked)
        - plotgrid -> Array of dt.plots (titles: list[str]) ax5.set_title( wenn nötig!
        - scatterkdeplot -> wie googleplaystore -> df_price_cleaned[df_price_cleaned[DOWNLOADS] < 2000000]

        + sorted_values -> bool
        + figsize + figsize_cat=[small, medium, large, xlarge]
        + rotate x/y
        + weitere gewollte features!
    

    PLOT.UTILS (einmal plot.py & plot as dir)
        ----------------
        from matplotlib.ticker import MaxNLocator
        def xaxis_to_integers(ax: any) -> None:
            ax.xaxis.set_major_locator(MaxNLocator(integer=True))

        def yaxis_to_integers(ax: any) -> None:
            ax.yaxis.set_major_locator(MaxNLocator(integer=True))
        ----------------
        EXPORT
        def get_fig_title(dirpath: str, plot_name: str) -> str:
            try:
                plot_count = len(glob(join(dirpath, "*.jpg"))) + 1
                count_prefix = zero_prefix(plot_count)

                if plot_name.strip():
                    plotname = plot_name.replace(" ", "_").lower()
                    return f"{count_prefix}_{plotname}"

                plotname = plt.gcf().axes[0].get_title()
                if plotname.strip():
                    plotname = plotname.replace(" ", "_").lower()
                    return f"{count_prefix}_{plotname}.jpg"

                return f"{count_prefix}_{uuid.uuid1().hex}.jpg"
            except:
                return f"{uuid.uuid1().hex}.jpg"

        def get_plot_dirpath(notebook_name="notebook1") -> str:
            return join(os.getcwd(), "plots", notebook_name)

        def saveplot(notebook_name="notebook1", plot_name="") -> str:
            dirpath = get_plot_dirpath(notebook_name)
            if not exists(dirpath):
                os.makedirs(dirpath)

            plotname = get_fig_title(dirpath, plot_name)
            full_plot_path = join(dirpath, plotname)

            plt.savefig(full_plot_path)
        ----------------
        def clear_plots_dir(notebook_name="notebook1") -> None:
            dirpath = get_plot_dirpath(notebook_name)
            if exists(dirpath):
                shutil.rmtree(dirpath)

        ----------------
        def plot_target_relation(column_name):
            _, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))
            ax1, ax2 = axes
            
            target_pvt = df.pivot_table(index=column_name, values=TARGET, aggfunc="mean").sort_index()

            sns.pointplot(x=TARGET, y=column_name, data=df, ax=ax1).set_title(f"Relation Hazardous and {column_name}")
            sns.regplot(x=target_pvt.index, y=target_pvt.to_numpy(), ci=False, line_kws={"color": "C1"}, scatter_kws={"alpha": 0.5}, ax=ax2).set_title(f"Linear Relation Hazardous and {column_name}")
        ----------------
        def plot_target_relation_cat(column_name):
            g = sns.pointplot(x=column_name, y=TARGET, data=df)
            g.set_title(f"Relation Hazardous-Mean and {column_name}")
            g.tick_params(axis='x', rotation=45)
        ----------------
        def plot_dist(column_name):
            _, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))
            ax1, ax2 = axes
            
            title = f"Distribution of {column_name}"
            ax1.set_title(title)
            ax2.set_title(title)

            sns.boxplot(x=column_name, data=df, ax=ax1)
            df[column_name].hist(ax=ax2)

            print(f"Skew: \t {df[column_name].skew()}")
        ----------------
        def plot_dist_cat(column_name):
            g = sns.countplot(x=column_name, data=df, color="C0")
            g.tick_params(axis='x', rotation=45)
            g.set_title(f"Distribution of {column_name}")

CONST (Konstanten für alle settings etc!)
    -> def func(method: Literal['simple_method', 'some_other_method']): => !!! USE THIS !!!
    -> OR:
    class FigSize(Enum):
        SMALL = 1
        MEDIUM = 2
        LARGE = 3
        XLARGE = 4

    def f(method: FigSize):
        if(method == FigSize.LARGE):
            print("IS_LARGE")
        else:
            print("ERR")

    f(FigSize.LARGE)

UTILS
    -> eine helfer-methode um alle spalten eines DF zu lesen und dann eine ausgabe mit der KonstantenName = columnName [TDD_TEST]
        - dann kann man die ausgabe kopieren und nochmal bearbeiten
        - UND: bei Großbuchstaben im string -> _ und    _ = _
                                                        - = _ 
                                                        . -> _ (außer am ende, dann weg)
                                                        und generell Sonderzeichen mit _ replacen! (regex)
    
    -> missingno in dt.msno... einbetten
    -> reorder columns function -> [add_col(data: Series, DaraFrame = None, to=index | colname = None, before=True), move_col(Series | DataFrame), to: int | str = 0]
        -> weitere?
    -> addRow
    -> addRows
    -> dt.heatmap
    -> dt.set() -> alle settings setzen (default wie gewollt)
    -> pvttbl_to_df (to_records)
    -> df_checkpoint
        - export eines df mit namen df_checkpoint_export (als further?)
        - returnt das gelesene df df_checkpoint_read (als further?)
    -> delete_checkpoints#
    -> PDF Reader -> Text and as image PyPDF2, pdf2image
    -> get_date_format("DD-MM-YYYY") or HELP_DATE_FORMATS -> print dateformats
    ----------------
    HOUR_MINUTES = 60
    def zero_prefix(number_text: str) -> str:
        return f"0{number_text}" if int(number_text) < 10 else f"{number_text}"

    # def hrstr(minutes, suffix="h", usfmt=False, milliseconds=False)
    def get_hours_string(minutes: float | int) -> str:
        hour_string  = zero_prefix(int(minutes / HOUR_MINUTES))
        minutes_string = zero_prefix(int(minutes % HOUR_MINUTES))
        return f"{hour_string}:{minutes_string}h"
    ----------------
    def to_category(df: pd.DataFrame, column: str, category_order: list | tuple):
        # validate...
        return df[column].astype(dt.const.CATEGORY).cat.reorder_categories(category_order, ordered=True)
    ----------------
    def df_checkpoint   

STATS
    -> Kontingenztabelle [TDD_TEST]
        - prop and abs (default -> prop)

PREPROCESSING
    -> read_csv => returns tuple -> raw_data_df and working_df
        - read_csv and then df.copy()
    -> numcolumns(ignore=[ID]) => df.drop(columns=[ID]).describe().columns
    -> catcolumns(ignore=[COL]) => df.drop(columns=[ID]).describe(include=["O"]).columns

DB
    -> DATABASE-Class
        -> mit SQL-Alchemy Engine für postgres, MySQL, MSSQL-SERVER, SQLITE
        -> query_executer
        -> mit pandas und ohne!
        -> engine in Class halten
    -> NoSQL connector für mongodb!

DECORATORS
    @zipparams
        ->  die parameter sind extrem viele. damit man mit diesen gut arbeiten kann,
            setzt man den decorator und dieser zipt alle parameter in ein dict -> zipped_params => ** zipped_params übergeben
            + zipped_params(ignore=[]) !!! ignorieren von parametern!

TYPING
    -> interfaces für rückgabewerte :)
        -> immer ergeben lassen, fehlendes typing hinzufügen und ggf. methoden überschreiben mit typing -> IDataFrame

--------------------------------------- ML & DL ---------------------------------------

plot
    DfClassificationPlotter(df, ignore=[col1, col2...])
        -> info()
        -> describe_nummeric
        -> describe_categorical
        -> isna
        -> nunique
        -> plot_missing
        -> plot_numeric_hist
        -> plot_numeric_pairplot
        -> plot_numeric_corr
        -> plot_numeric_dist(colname)
        -> plot_categorical_dist(colname) (skew, hist)
        -> plot_numeric_target_relation
        -> plot_categorical_target_relation
        -> plot_target_dist

PREPROCESSING
    -> feature_target_split(df: DataFrame, target: str) -> tuple[X, y]
    -> train_test_validation_split()
    -> train_test_validation_split_raw(df: DataFrame, target: str, test_size, val_size, radom_state etc)
        -> incl. feat_tar_split
    -> log_transform
        with fillna(0) after log10() 
        def log_transform(df: DataFrame):
            return np.log10(df)
    -> scale(Scaler: IScaler, df_train: DataFrame, columns: list[str] df_test: DataFrame = None) -> DataFrame | tuple[DataFrame]
    -> borderline_smote(df: DataFrame, target: str)
        -> to numpy
        -> BorderlineSmote...
        -> to df back
    -> pd.get_dummies in better!


UTILS
    -> print_df_head(start_col_idx)
    -> print_shapes(...: DataFrame | Series | np.array) (nur: dt.shapes)
    -> savenpz, savefurther | loadnpz, loadfurther (inkl. colnames?)
    ---------------------------------------------
    -> savemodel, loadmodel (model: IModel, path, filename, timestamp: bool = True)
    def export_model(model, model_name):
    timestamp_string = str(datetime.now())[:19].replace(" ", "").replace(":", "").replace("-", "")
    export_path = f"../trained_models/{timestamp_string}_{model_name}"
    with open(export_path, "wb") as model_file:
        pickle.dump(model, model_file)
    
    return export_path
    ---------------------------------------------

def load_model(model_path):
    with open(model_path, "rb") as model_file:
        return pickle.load(model_file)



EVALUATION
    -> print_classification_report -> print_model_classification_report (noch verfeinern!)
    -> print_feature_importance
    -> print_classfification_eval_metrics
    ------------------------------------------
    -> print_top_rankings (gridserach)
    def print_top_rankings(grid_model, top=10):
    sorted_zipped_report = sorted(
        zip(
            grid_model.cv_results_["params"], 
            grid_model.cv_results_["mean_test_score"], 
            grid_model.cv_results_["rank_test_score"]
        ), key=lambda item: item[2]
    )
    for (i, (params, score, rank)) in enumerate(sorted_zipped_report):
        if i >= top:
            break

        print(f"Rank: {rank}\tScore: {score}\nParam: {params}\n")
    ------------------------------------------
    -> print_model_scores(models: list[IModel], metric="f1"|"accuracy"|"recall"..., X) -> Series
    recall_models = Series(
    data=[
        recall_score(y_test, knn_best_model.predict(X_test)),
        recall_score(y_test, rf_best_model.predict(X_test)),
        recall_score(y_test, svm_best_model.predict(X_test)),
        recall_score(y_test, xgb_best_model.predict(X_test)),
    ],
    index=["KNN", "RandomForest", "SVM", "XGBoost"] -> names.UPPER und möglichkeit array
)

recall_models.sort_values(ascending=False)
------------------------------------------
-> print_score(Score: callable, X, y, decimals: int = 3)
    f"{np.round(f1_score(y_val, MODEL_PREDICTIONS) * 100, 3)}%"


MODELING
    -> fast_grid_search (incl_voting_classifier: boolean = True)
        -> erstmal testen, ob das mit wenig daten überhaupt geht...
        -> mehrere modellarten -> modelle vorgeben und mit bool ein/ausschalten!
        -> mit weiteren modellen erweiterbar machen -> liste aus FGSModel(modellclass, ...params)
        -> 
        -> inkl. uuid-log.txt
    
    -> IModel, Model(IModel)
    class Model(IModel):
     def __init__(self, model, scaler):
         self.model = model
         self.scaler = scaler
   
     def predict(self, values):
         prep_values = self.preprocess_values(values)
         pass

     def preprocess_values(self, values):
         pass
